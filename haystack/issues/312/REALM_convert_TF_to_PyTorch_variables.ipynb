{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "REALM-convert_TF_to_PyTorch_variables.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTjhAMa8iMgO"
      },
      "source": [
        "# Install transformers library to use DPR classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrlCjGIxiKL1"
      },
      "source": [
        "! git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFwPWLQDiL6P"
      },
      "source": [
        "%cd transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHarkECyiMDM"
      },
      "source": [
        "! pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nocs-Gh_imOg"
      },
      "source": [
        "# Download REALM models (in Tensorflow)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx-Df8jXeczK"
      },
      "source": [
        "! mkdir download\n",
        "! gsutil -m cp -R gs://realm-data/cc_news_pretrained/embedder/ download\n",
        "! gsutil -m cp -R gs://realm-data/orqa_nq_model_from_realm/ download\n",
        "! ls download/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzlLP0bwl1DH"
      },
      "source": [
        "# Context Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV6GpKRnFBVs"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "embedder_model_or_path=\"download/embedder\"\n",
        "document_embedder_model = tf.saved_model.load_v2(embedder_model_or_path, tags={})"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dugLPtyGFJEM"
      },
      "source": [
        "variable_names=[el.name for el in document_embedder_model.variables]\n",
        "my_variables_embedder=sorted(variable_names)\n",
        "\n",
        "print(len(my_variables_embedder))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewBh_gdznqC9"
      },
      "source": [
        "my_variables_embedder[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECKs_bmxl44r"
      },
      "source": [
        "#############################################################################################################################\n",
        "#############################################################################################################################\n",
        "#############################################################################################################################\n",
        "#############################################################################################################################\n",
        "#STEP 1\n",
        "\n",
        "import collections\n",
        "import torch\n",
        "\n",
        "model_dict_TF=collections.OrderedDict()\n",
        "\n",
        "for el in document_embedder_model.variables:\n",
        "  name=el.name[len(\"module/module/\"):] \n",
        "  model_dict_TF[name]=torch.from_numpy(el.numpy()) #i.e. vectors are Torch.tensor instances in PyTorch"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O98gdiKPqvKd"
      },
      "source": [
        "#############################################################################################################################\n",
        "#############################################################################################################################\n",
        "#############################################################################################################################\n",
        "#############################################################################################################################\n",
        "#STEP 2\n",
        "\n",
        "final_model_dict=collections.OrderedDict()\n",
        "\n",
        "\n",
        "prefix=\"module/bert/\"\n",
        "\n",
        "for key, value in model_dict_TF.items():\n",
        "  if key.startswith(prefix):\n",
        "    key = key[len(prefix) :]\n",
        "  \n",
        "  key = key.replace(\"layer_\",\"layer.\").replace(\"/\",\".\").replace(\"gamma:0\",\"weight\").replace(\"beta:0\",\"bias\").replace(\"kernel:0\",\"weight\").replace(\"bias:0\",\"bias\")\n",
        "  \n",
        "  if \"module.cls.predictions\" in key:\n",
        "    continue #SKIPPING - could they be useful?\n",
        "\n",
        "  if key == \"LayerNorm.bias\" or key == \"LayerNorm.weight\":\n",
        "    continue #SKIPPING\n",
        "\n",
        "  if key == \"embeddings.word_embeddings:0\":\n",
        "    key=\"embeddings.word_embeddings.weight\"\n",
        "\n",
        "  if key == \"embeddings.token_type_embeddings:0\":\n",
        "    key=\"embeddings.token_type_embeddings.weight\"\n",
        "\n",
        "  if key == \"embeddings.position_embeddings:0\":\n",
        "    key=\"embeddings.position_embeddings.weight\"\n",
        "\n",
        "\n",
        "  if \"intermediate.dense.weight\" in key or \"output.dense.weight\" in key:\n",
        "    value=value.T\n",
        "\n",
        "  prefix2=\"bert_model.\"\n",
        "  key=prefix2+key\n",
        "\n",
        "\n",
        "  #useful to project from 768 to 128\n",
        "  if key == prefix2+\"dense.weight\":\n",
        "    key=\"encode_proj.weight\"  \n",
        "    value=value.T\n",
        "    \n",
        "  if key == prefix2+\"dense.bias\":\n",
        "    key=\"encode_proj.bias\"   \n",
        "\n",
        "  final_model_dict[key]=value "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK2iQ3E1qVGO"
      },
      "source": [
        "len(final_model_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y9RiJFwqVQK"
      },
      "source": [
        "list(final_model_dict.keys())[-10:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iavh9Wy7RBDC"
      },
      "source": [
        "torch.save(final_model_dict, \"pytorch_model.bin\") #At this point, I can save weights using PyTorch"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkFzDQSFRBMb"
      },
      "source": [
        "! mkdir document_checkpoint_REALM\n",
        "! mv pytorch_model.bin document_checkpoint_REALM"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s90eDEvNYv1X"
      },
      "source": [
        "#Original config downloaded from this link: https://s3.amazonaws.com/models.huggingface.co/bert/facebook/dpr-ctx_encoder-single-nq-base/config.json\n",
        "#I only changed projection_dim from 0 to 128 because in REALM you have 128-dim embeddings\n",
        "\n",
        "config={\n",
        "  \"architectures\": [\n",
        "    \"DPRContextEncoder\"\n",
        "  ],\n",
        "  \"attention_probs_dropout_prob\": 0.1,\n",
        "  \"gradient_checkpointing\": False,\n",
        "  \"hidden_act\": \"gelu\",\n",
        "  \"hidden_dropout_prob\": 0.1,\n",
        "  \"hidden_size\": 768,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"intermediate_size\": 3072,\n",
        "  \"layer_norm_eps\": 1e-12,\n",
        "  \"max_position_embeddings\": 512,\n",
        "  \"model_type\": \"dpr\",\n",
        "  \"num_attention_heads\": 12,\n",
        "  \"num_hidden_layers\": 12,\n",
        "  \"pad_token_id\": 0,\n",
        "  \"projection_dim\": 128,\n",
        "  \"type_vocab_size\": 2,\n",
        "  \"vocab_size\": 30522\n",
        "}\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "with open('document_checkpoint_REALM/config.json', 'w') as fp:\n",
        "    json.dump(config, fp)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6P4DY33YwJV"
      },
      "source": [
        "from transformers import DPRContextEncoder\n",
        "\n",
        "#Now I \"embed\" REALM context encoder into a DPR context encoder\n",
        "ctx_encoder = DPRContextEncoder.from_pretrained(\"document_checkpoint_REALM\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4Fs65MRl3mx"
      },
      "source": [
        "# Question Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBGQ3wV8ZsZ9"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "best_model_dir=\"download/orqa_nq_model_from_realm\"\n",
        "\n",
        "best_checkpoint_pattern = os.path.join(best_model_dir, \"export\",\n",
        "                                        \"best_default\", \"checkpoint\",\n",
        "                                        \"*.index\")\n",
        "best_checkpoint = tf.io.gfile.glob(\n",
        "    best_checkpoint_pattern)[0][:-len(\".index\")]\n",
        "\n",
        "best_model_REALM_NQ=tf.train.load_checkpoint(best_checkpoint)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv40QiJtZsj8"
      },
      "source": [
        "ris=best_model_REALM_NQ.get_variable_to_dtype_map()\n",
        "my_list=[i for i in ris]\n",
        "my_variables_nq_model=sorted(my_list)\n",
        "\n",
        "len(my_variables_nq_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYhM5U2Jlo66"
      },
      "source": [
        "my_variables_nq_model[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3DFJwUNmnUG"
      },
      "source": [
        "#############################################################################################################################\n",
        "#############################################################################################################################\n",
        "#############################################################################################################################\n",
        "#############################################################################################################################\n",
        "#STEP 1\n",
        "\n",
        "def delete_useless_variables(my_variables_nq_model):\n",
        "  useful_variables=[]\n",
        "  for variable in my_variables_nq_model:\n",
        "    if variable == \"global_step\" or \"reader\" in variable or \"adam\" in variable:\n",
        "      continue\n",
        "    useful_variables.append(variable)\n",
        "\n",
        "  return useful_variables\n",
        "\n",
        "import collections\n",
        "import torch\n",
        "\n",
        "my_variables_nq_model_mod=delete_useless_variables(my_variables_nq_model)\n",
        "\n",
        "model_dict_TF=collections.OrderedDict()\n",
        "\n",
        "for variable in my_variables_nq_model_mod:\n",
        "  name=variable[len(\"module/module/module/\"):] \n",
        "  model_dict_TF[name]=torch.from_numpy(best_model_REALM_NQ.get_tensor(variable)) #i.e. vectors are Torch.tensor instances in PyTorch"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxccGl5mmcba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6f5b33c-07ea-4edb-8362-89685ca5a35f"
      },
      "source": [
        "len(model_dict_TF)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "208"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1-bdf16mhpD"
      },
      "source": [
        "#############################################################################################################################\n",
        "#############################################################################################################################\n",
        "#############################################################################################################################\n",
        "#############################################################################################################################\n",
        "#STEP 2\n",
        "\n",
        "final_model_dict=collections.OrderedDict()\n",
        "\n",
        "\n",
        "prefix=\"module/bert/\"\n",
        "\n",
        "for key, value in model_dict_TF.items():\n",
        "  if key.startswith(prefix):\n",
        "    key = key[len(prefix) :]\n",
        "  \n",
        "  key = key.replace(\"layer_\",\"layer.\").replace(\"/\",\".\").replace(\"gamma\",\"weight\").replace(\"beta\",\"bias\").replace(\"kernel\",\"weight\")\n",
        "  \n",
        "  if \"module.cls.predictions\" in key:\n",
        "    continue #SKIPPING - could they be useful?\n",
        "\n",
        "  if key == \"LayerNorm.bias\" or key == \"LayerNorm.weight\":\n",
        "    continue #SKIPPING\n",
        "\n",
        "\n",
        "  if key == \"embeddings.word_embeddings\": #NB: here there is no \":0\" compared to context encoder conversion \n",
        "    key=\"embeddings.word_embeddings.weight\"\n",
        "\n",
        "  if key == \"embeddings.token_type_embeddings\": #NB: here there is no \":0\" compared to context encoder conversion \n",
        "    key=\"embeddings.token_type_embeddings.weight\"\n",
        "\n",
        "  if key == \"embeddings.position_embeddings\": #NB: here there is no \":0\" compared to context encoder conversion\n",
        "    key=\"embeddings.position_embeddings.weight\"\n",
        "\n",
        "\n",
        "  if \"intermediate.dense.weight\" in key or \"output.dense.weight\" in key:\n",
        "    value=value.T\n",
        "\n",
        "  prefix2=\"bert_model.\"\n",
        "  key=prefix2+key\n",
        "\n",
        "\n",
        "  #useful to project from 768 to 128\n",
        "  if key == prefix2+\"dense.weight\":\n",
        "    key=\"encode_proj.weight\"  \n",
        "    value=value.T\n",
        "    \n",
        "  if key == prefix2+\"dense.bias\":\n",
        "    key=\"encode_proj.bias\"\n",
        "\n",
        "  final_model_dict[key]=value"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urd9HAGHqVnU"
      },
      "source": [
        "len(final_model_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L07hMyKHs85A"
      },
      "source": [
        "list(final_model_dict.keys())[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HO-g_A7gs9Ez"
      },
      "source": [
        "torch.save(final_model_dict, \"pytorch_model.bin\") #At this point, I can save weights using PyTorch"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9_76iubtIbS"
      },
      "source": [
        "! mkdir question_checkpoint_REALM\n",
        "! mv pytorch_model.bin question_checkpoint_REALM"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv-qGnFBtPZn"
      },
      "source": [
        "#Original config downloaded from this link: https://s3.amazonaws.com/models.huggingface.co/bert/facebook/dpr-question_encoder-single-nq-base/config.json\n",
        "#I only changed projection_dim from 0 to 128 because in REALM you have 128-dim embeddings\n",
        "\n",
        "config={\n",
        "  \"architectures\": [\n",
        "    \"DPRQuestionEncoder\"\n",
        "  ],\n",
        "  \"attention_probs_dropout_prob\": 0.1,\n",
        "  \"gradient_checkpointing\": False,\n",
        "  \"hidden_act\": \"gelu\",\n",
        "  \"hidden_dropout_prob\": 0.1,\n",
        "  \"hidden_size\": 768,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"intermediate_size\": 3072,\n",
        "  \"layer_norm_eps\": 1e-12,\n",
        "  \"max_position_embeddings\": 512,\n",
        "  \"model_type\": \"dpr\",\n",
        "  \"num_attention_heads\": 12,\n",
        "  \"num_hidden_layers\": 12,\n",
        "  \"pad_token_id\": 0,\n",
        "  \"projection_dim\": 128,\n",
        "  \"type_vocab_size\": 2,\n",
        "  \"vocab_size\": 30522\n",
        "}\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "with open('question_checkpoint_REALM/config.json', 'w') as fp:\n",
        "    json.dump(config, fp)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWRwpBG4tRhU"
      },
      "source": [
        "from transformers import DPRQuestionEncoder\n",
        "\n",
        "#Now I \"embed\" REALM question encoder into a DPR question encoder\n",
        "question_encoder = DPRQuestionEncoder.from_pretrained(\"question_checkpoint_REALM\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QoOAMMVsHQE"
      },
      "source": [
        "# Save models in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p-_wxSnFY1l"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/mnt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk7K_QqkDFip"
      },
      "source": [
        "! mkdir \"/content/mnt/My Drive/REALM_retrieval_models/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_z-I00dsQdV"
      },
      "source": [
        "! cp -r document_checkpoint_REALM \"/content/mnt/My Drive/REALM_retrieval_models/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cy3I9KAsnYa"
      },
      "source": [
        "! cp -r question_checkpoint_REALM \"/content/mnt/My Drive/REALM_retrieval_models/\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}