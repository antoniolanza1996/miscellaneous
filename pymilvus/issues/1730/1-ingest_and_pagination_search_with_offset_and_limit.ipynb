{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain==0.0.291\n",
      "openai==0.28.0\n",
      "pypdf==3.16.0\n",
      "tiktoken==0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep langchain\n",
    "!pip freeze | grep openai\n",
    "!pip freeze | grep pypdf\n",
    "!pip freeze | grep tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import DeterministicFakeEmbedding\n",
    "\n",
    "vector_dim = 768\n",
    "embeddings = DeterministicFakeEmbedding(size=vector_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Milvus\n",
    "\n",
    "DEFAULT_MILVUS_CONNECTION = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"19530\",\n",
    "    \"user\": \"\",\n",
    "    \"password\": \"\",\n",
    "    \"secure\": False,\n",
    "}\n",
    "\n",
    "collection_name = f\"our_20231011_collection\"\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=collection_name,\n",
    "    connection_args=DEFAULT_MILVUS_CONNECTION,\n",
    "    consistency_level=\"Session\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "NUM_CHUNKS = 10000\n",
    "\n",
    "documents = []\n",
    "for num_c in range(NUM_CHUNKS):\n",
    "    documents.append(\n",
    "        Document(\n",
    "            page_content=f\"my text for chunk {num_c}\",\n",
    "            metadata={\n",
    "                'chunk': num_c\n",
    "            }\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting 10000 chunks of 768 dimension\n",
      "Creating a new collection\n",
      "Creating a new partition\n",
      "vector_store.add_documents execution_time=109.3074236249995s\n",
      "CPU times: user 3.04 s, sys: 244 ms, total: 3.28 s\n",
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import random\n",
    "from pymilvus import Collection\n",
    "import time\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "kb_id = \"my_kb_15\"\n",
    "\n",
    "print(f\"Ingesting {len(documents)} chunks of {vector_dim} dimension\")\n",
    "\n",
    "if len(documents) == 0:\n",
    "    raise ValueError()\n",
    "\n",
    "# If the collection hasn't been initialized yet, perform all steps to do so...\n",
    "if not isinstance(vector_store.col, Collection):\n",
    "    print(\"Creating a new collection\")\n",
    "    vector_store._init(\n",
    "        embeddings.embed_documents([documents[0].page_content]),\n",
    "        [documents[0].metadata]\n",
    "    )\n",
    "\n",
    "\n",
    "# if the current collection doesn't have a partition for this KB_ID, let's create now...\n",
    "if not vector_store.col.has_partition(partition_name=kb_id):\n",
    "    print(\"Creating a new partition\")\n",
    "    vector_store.col.create_partition(partition_name=kb_id)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "_ = vector_store.add_documents(\n",
    "    documents,\n",
    "    partition_name=kb_id,\n",
    ")\n",
    "print(f\"vector_store.add_documents execution_time={time.perf_counter()-start_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_page=1 with offset=0\n",
      "\t1) chunk_id=464, score=1276.47021484375, text=my text for chunk 464...\n",
      "\t2) chunk_id=7775, score=1315.8236083984375, text=my text for chunk 7775...\n",
      "\t3) chunk_id=8206, score=1325.511962890625, text=my text for chunk 8206...\n",
      "\t4) chunk_id=9671, score=1326.6737060546875, text=my text for chunk 9671...\n",
      "\t5) chunk_id=1401, score=1329.3077392578125, text=my text for chunk 1401...\n",
      "curr_page=2 with offset=5\n",
      "\t6) chunk_id=4236, score=1330.72705078125, text=my text for chunk 4236...\n",
      "\t7) chunk_id=3705, score=1332.610595703125, text=my text for chunk 3705...\n",
      "\t8) chunk_id=5903, score=1333.90966796875, text=my text for chunk 5903...\n",
      "\t9) chunk_id=1145, score=1336.4095458984375, text=my text for chunk 1145...\n",
      "\t10) chunk_id=6258, score=1342.9508056640625, text=my text for chunk 6258...\n",
      "curr_page=3 with offset=10\n",
      "\t11) chunk_id=4290, score=1349.108154296875, text=my text for chunk 4290...\n",
      "\t12) chunk_id=981, score=1350.4874267578125, text=my text for chunk 981...\n",
      "\t13) chunk_id=3000, score=1350.5401611328125, text=my text for chunk 3000...\n",
      "\t14) chunk_id=5493, score=1351.693115234375, text=my text for chunk 5493...\n",
      "\t15) chunk_id=7319, score=1354.885498046875, text=my text for chunk 7319...\n",
      "curr_page=4 with offset=15\n",
      "\t16) chunk_id=7530, score=1346.193359375, text=my text for chunk 7530...\n",
      "\t17) chunk_id=1103, score=1348.271484375, text=my text for chunk 1103...\n",
      "\t18) chunk_id=4290, score=1349.108154296875, text=my text for chunk 4290...\n",
      "\t19) chunk_id=981, score=1350.4874267578125, text=my text for chunk 981...\n",
      "\t20) chunk_id=3000, score=1350.5401611328125, text=my text for chunk 3000...\n",
      "curr_page=5 with offset=20\n",
      "\t21) chunk_id=544, score=1351.292236328125, text=my text for chunk 544...\n",
      "\t22) chunk_id=5493, score=1351.693115234375, text=my text for chunk 5493...\n",
      "\t23) chunk_id=7319, score=1354.885498046875, text=my text for chunk 7319...\n",
      "\t24) chunk_id=157, score=1356.572265625, text=my text for chunk 157...\n",
      "\t25) chunk_id=4664, score=1358.426513671875, text=my text for chunk 4664...\n",
      "curr_page=6 with offset=25\n",
      "\t26) chunk_id=157, score=1356.572265625, text=my text for chunk 157...\n",
      "\t27) chunk_id=4664, score=1358.426513671875, text=my text for chunk 4664...\n",
      "\t28) chunk_id=2667, score=1359.5396728515625, text=my text for chunk 2667...\n",
      "\t29) chunk_id=1558, score=1360.955078125, text=my text for chunk 1558...\n",
      "\t30) chunk_id=7408, score=1360.962158203125, text=my text for chunk 7408...\n",
      "curr_page=7 with offset=30\n",
      "\t31) chunk_id=6997, score=1364.010986328125, text=my text for chunk 6997...\n",
      "\t32) chunk_id=4034, score=1364.259033203125, text=my text for chunk 4034...\n",
      "\t33) chunk_id=5420, score=1365.097412109375, text=my text for chunk 5420...\n",
      "\t34) chunk_id=9223, score=1368.972412109375, text=my text for chunk 9223...\n",
      "\t35) chunk_id=4946, score=1369.449951171875, text=my text for chunk 4946...\n",
      "curr_page=8 with offset=35\n",
      "\t36) chunk_id=7756, score=1371.405517578125, text=my text for chunk 7756...\n",
      "\t37) chunk_id=9867, score=1371.4217529296875, text=my text for chunk 9867...\n",
      "\t38) chunk_id=7575, score=1371.72216796875, text=my text for chunk 7575...\n",
      "\t39) chunk_id=4162, score=1372.3555908203125, text=my text for chunk 4162...\n",
      "\t40) chunk_id=248, score=1373.0211181640625, text=my text for chunk 248...\n",
      "curr_page=9 with offset=40\n",
      "\t41) chunk_id=7575, score=1371.72216796875, text=my text for chunk 7575...\n",
      "\t42) chunk_id=4162, score=1372.3555908203125, text=my text for chunk 4162...\n",
      "\t43) chunk_id=248, score=1373.0211181640625, text=my text for chunk 248...\n",
      "\t44) chunk_id=5143, score=1373.8109130859375, text=my text for chunk 5143...\n",
      "\t45) chunk_id=8814, score=1375.21630859375, text=my text for chunk 8814...\n",
      "curr_page=10 with offset=45\n",
      "\t46) chunk_id=2755, score=1375.3636474609375, text=my text for chunk 2755...\n",
      "\t47) chunk_id=6829, score=1376.0576171875, text=my text for chunk 6829...\n",
      "\t48) chunk_id=888, score=1376.187255859375, text=my text for chunk 888...\n",
      "\t49) chunk_id=5638, score=1376.48583984375, text=my text for chunk 5638...\n",
      "\t50) chunk_id=8677, score=1379.80078125, text=my text for chunk 8677...\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "search_params = {\n",
    "    'offset': 0\n",
    "}\n",
    "\n",
    "curr_page = 0\n",
    "MAX_PAGES = 10\n",
    "\n",
    "scores = []\n",
    "chunks_ids = []\n",
    "query = \"my query\"\n",
    "while curr_page < MAX_PAGES:\n",
    "    print(f\"curr_page={curr_page+1} with offset={search_params['offset']}\")\n",
    "    output = vector_store.similarity_search_with_score(\n",
    "        query=query,\n",
    "        k=k,\n",
    "        param=search_params,\n",
    "        # expr=expr,\n",
    "        partition_names=[kb_id],\n",
    "    )\n",
    "    for i, chunk in enumerate(output):\n",
    "        text = chunk[0].page_content[:100].replace('\\n', ' ')\n",
    "        print(f\"\\t{search_params['offset'] + i+1}) chunk_id={chunk[0].metadata['chunk']}, score={chunk[1]}, text={text}...\")\n",
    "        scores.append(chunk[1])\n",
    "        chunks_ids.append(chunk[0].metadata['chunk'])\n",
    "    curr_page+=1\n",
    "    search_params['offset'] = search_params['offset'] + k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/antonio/Desktop/milvus/notebooks/8-ingest_and_pagination_search_with_offset_and_limit.ipynb Cella 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/antonio/Desktop/milvus/notebooks/8-ingest_and_pagination_search_with_offset_and_limit.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m Counter\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/antonio/Desktop/milvus/notebooks/8-ingest_and_pagination_search_with_offset_and_limit.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m Counter(chunks_ids)\u001b[39m.\u001b[39mmost_common()[:\u001b[39m4\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/antonio/Desktop/milvus/notebooks/8-ingest_and_pagination_search_with_offset_and_limit.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(chunks_ids) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(chunks_ids))\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(chunks_ids).most_common()[:4]\n",
    "assert len(chunks_ids) == len(set(chunks_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "15 1354.885498046875, 1346.193359375",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/antonio/Desktop/milvus/notebooks/8-ingest_and_pagination_search_with_offset_and_limit.ipynb Cella 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/antonio/Desktop/milvus/notebooks/8-ingest_and_pagination_search_with_offset_and_limit.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(scores)):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/antonio/Desktop/milvus/notebooks/8-ingest_and_pagination_search_with_offset_and_limit.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39massert\u001b[39;00m scores[i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m scores[i], \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mscores[i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mscores[i]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 15 1354.885498046875, 1346.193359375"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(scores)):\n",
    "    assert scores[i-1] <= scores[i], f\"{i} {scores[i-1]}, {scores[i]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "milvus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
