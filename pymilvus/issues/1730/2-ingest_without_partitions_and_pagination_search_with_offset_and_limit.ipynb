{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain==0.0.291\n",
      "openai==0.28.0\n",
      "pypdf==3.16.0\n",
      "tiktoken==0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep langchain\n",
    "!pip freeze | grep openai\n",
    "!pip freeze | grep pypdf\n",
    "!pip freeze | grep tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import DeterministicFakeEmbedding\n",
    "\n",
    "vector_dim = 768\n",
    "embeddings = DeterministicFakeEmbedding(size=vector_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Milvus\n",
    "\n",
    "DEFAULT_MILVUS_CONNECTION = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"19530\",\n",
    "    \"user\": \"\",\n",
    "    \"password\": \"\",\n",
    "    \"secure\": False,\n",
    "}\n",
    "\n",
    "collection_name = f\"our_20231011_collection_no_partitions\"\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=collection_name,\n",
    "    connection_args=DEFAULT_MILVUS_CONNECTION,\n",
    "    consistency_level=\"Session\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "NUM_CHUNKS = 10000\n",
    "\n",
    "documents = []\n",
    "for num_c in range(NUM_CHUNKS):\n",
    "    documents.append(\n",
    "        Document(\n",
    "            page_content=f\"my text for chunk {num_c}\",\n",
    "            metadata={\n",
    "                'chunk': num_c\n",
    "            }\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting 10000 chunks of 768 dimension\n",
      "vector_store.add_documents execution_time=22.75188058299682s\n",
      "CPU times: user 3.1 s, sys: 163 ms, total: 3.27 s\n",
      "Wall time: 22.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import random\n",
    "from pymilvus import Collection\n",
    "import time\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "print(f\"Ingesting {len(documents)} chunks of {vector_dim} dimension\")\n",
    "\n",
    "if len(documents) == 0:\n",
    "    raise ValueError()\n",
    "\n",
    "\"\"\"# If the collection hasn't been initialized yet, perform all steps to do so...\n",
    "if not isinstance(vector_store.col, Collection):\n",
    "    print(\"Creating a new collection\")\n",
    "    vector_store._init(\n",
    "        embeddings.embed_documents([documents[0].page_content]),\n",
    "        [documents[0].metadata]\n",
    "    )\n",
    "\"\"\"\n",
    "start_time = time.perf_counter()\n",
    "_ = vector_store.add_documents(\n",
    "    documents,\n",
    ")\n",
    "print(f\"vector_store.add_documents execution_time={time.perf_counter()-start_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_page=1 with offset=0\n",
      "\t1) chunk_id=1417, score=1282.739501953125, text=my text for chunk 1417...\n",
      "\t2) chunk_id=7990, score=1294.706787109375, text=my text for chunk 7990...\n",
      "\t3) chunk_id=5123, score=1296.0478515625, text=my text for chunk 5123...\n",
      "\t4) chunk_id=8755, score=1299.026123046875, text=my text for chunk 8755...\n",
      "\t5) chunk_id=6416, score=1307.1116943359375, text=my text for chunk 6416...\n",
      "curr_page=2 with offset=5\n",
      "\t6) chunk_id=9620, score=1313.47900390625, text=my text for chunk 9620...\n",
      "\t7) chunk_id=7775, score=1315.8236083984375, text=my text for chunk 7775...\n",
      "\t8) chunk_id=8851, score=1321.05029296875, text=my text for chunk 8851...\n",
      "\t9) chunk_id=4001, score=1321.920166015625, text=my text for chunk 4001...\n",
      "\t10) chunk_id=8206, score=1325.511962890625, text=my text for chunk 8206...\n",
      "curr_page=3 with offset=10\n",
      "\t11) chunk_id=598, score=1326.9158935546875, text=my text for chunk 598...\n",
      "\t12) chunk_id=1218, score=1328.949462890625, text=my text for chunk 1218...\n",
      "\t13) chunk_id=1401, score=1329.3077392578125, text=my text for chunk 1401...\n",
      "\t14) chunk_id=4236, score=1330.72705078125, text=my text for chunk 4236...\n",
      "\t15) chunk_id=3705, score=1332.610595703125, text=my text for chunk 3705...\n",
      "curr_page=4 with offset=15\n",
      "\t16) chunk_id=5903, score=1333.90966796875, text=my text for chunk 5903...\n",
      "\t17) chunk_id=4523, score=1334.219482421875, text=my text for chunk 4523...\n",
      "\t18) chunk_id=7228, score=1337.021728515625, text=my text for chunk 7228...\n",
      "\t19) chunk_id=4408, score=1338.329345703125, text=my text for chunk 4408...\n",
      "\t20) chunk_id=7009, score=1339.275634765625, text=my text for chunk 7009...\n",
      "curr_page=5 with offset=20\n",
      "\t21) chunk_id=6291, score=1341.508056640625, text=my text for chunk 6291...\n",
      "\t22) chunk_id=3058, score=1342.487548828125, text=my text for chunk 3058...\n",
      "\t23) chunk_id=5176, score=1344.044921875, text=my text for chunk 5176...\n",
      "\t24) chunk_id=6600, score=1345.56201171875, text=my text for chunk 6600...\n",
      "\t25) chunk_id=7530, score=1346.193359375, text=my text for chunk 7530...\n",
      "curr_page=6 with offset=25\n",
      "\t26) chunk_id=6778, score=1346.9512939453125, text=my text for chunk 6778...\n",
      "\t27) chunk_id=1406, score=1350.478759765625, text=my text for chunk 1406...\n",
      "\t28) chunk_id=3000, score=1350.5401611328125, text=my text for chunk 3000...\n",
      "\t29) chunk_id=544, score=1351.292236328125, text=my text for chunk 544...\n",
      "\t30) chunk_id=5493, score=1351.693115234375, text=my text for chunk 5493...\n",
      "curr_page=7 with offset=30\n",
      "\t31) chunk_id=7319, score=1354.885498046875, text=my text for chunk 7319...\n",
      "\t32) chunk_id=4510, score=1356.192626953125, text=my text for chunk 4510...\n",
      "\t33) chunk_id=6343, score=1357.0565185546875, text=my text for chunk 6343...\n",
      "\t34) chunk_id=1455, score=1357.4140625, text=my text for chunk 1455...\n",
      "\t35) chunk_id=8661, score=1358.287841796875, text=my text for chunk 8661...\n",
      "curr_page=8 with offset=35\n",
      "\t36) chunk_id=4664, score=1358.426513671875, text=my text for chunk 4664...\n",
      "\t37) chunk_id=883, score=1358.5682373046875, text=my text for chunk 883...\n",
      "\t38) chunk_id=3270, score=1359.032470703125, text=my text for chunk 3270...\n",
      "\t39) chunk_id=9204, score=1359.21875, text=my text for chunk 9204...\n",
      "\t40) chunk_id=2322, score=1359.508544921875, text=my text for chunk 2322...\n",
      "curr_page=9 with offset=40\n",
      "\t41) chunk_id=2667, score=1359.5396728515625, text=my text for chunk 2667...\n",
      "\t42) chunk_id=2168, score=1359.6800537109375, text=my text for chunk 2168...\n",
      "\t43) chunk_id=6262, score=1359.830078125, text=my text for chunk 6262...\n",
      "\t44) chunk_id=1558, score=1360.955078125, text=my text for chunk 1558...\n",
      "\t45) chunk_id=3441, score=1361.202392578125, text=my text for chunk 3441...\n",
      "curr_page=10 with offset=45\n",
      "\t46) chunk_id=6431, score=1362.342041015625, text=my text for chunk 6431...\n",
      "\t47) chunk_id=205, score=1362.92724609375, text=my text for chunk 205...\n",
      "\t48) chunk_id=6997, score=1364.010986328125, text=my text for chunk 6997...\n",
      "\t49) chunk_id=4034, score=1364.259033203125, text=my text for chunk 4034...\n",
      "\t50) chunk_id=1072, score=1364.49267578125, text=my text for chunk 1072...\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "search_params = {\n",
    "    'offset': 0\n",
    "}\n",
    "\n",
    "curr_page = 0\n",
    "MAX_PAGES = 10\n",
    "\n",
    "scores = []\n",
    "chunks_ids = []\n",
    "query = \"my query\"\n",
    "while curr_page < MAX_PAGES:\n",
    "    print(f\"curr_page={curr_page+1} with offset={search_params['offset']}\")\n",
    "    output = vector_store.similarity_search_with_score(\n",
    "        query=query,\n",
    "        k=k,\n",
    "        param=search_params,\n",
    "        # expr=expr,\n",
    "    )\n",
    "    for i, chunk in enumerate(output):\n",
    "        text = chunk[0].page_content[:100].replace('\\n', ' ')\n",
    "        print(f\"\\t{search_params['offset'] + i+1}) chunk_id={chunk[0].metadata['chunk']}, score={chunk[1]}, text={text}...\")\n",
    "        scores.append(chunk[1])\n",
    "        chunks_ids.append(chunk[0].metadata['chunk'])\n",
    "    curr_page+=1\n",
    "    search_params['offset'] = search_params['offset'] + k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(chunks_ids).most_common()[:4]\n",
    "assert len(chunks_ids) == len(set(chunks_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, len(scores)):\n",
    "    assert scores[i-1] <= scores[i], f\"{i} {scores[i-1]}, {scores[i]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "milvus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
